{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hrna_QjHg0lz"
      },
      "source": [
        "# Phi-2 GRPO Fine-tuning on Google Colab\n",
        "\n",
        "This notebook fine-tunes the Microsoft Phi-2 model using GRPO (Group Relative Policy Optimization) on the OpenAssistant/oasst1 dataset with QLoRA 4-bit quantization.\n",
        "\n",
        "**Optimized for T4 GPU (16GB VRAM)**\n",
        "\n",
        "## Setup Instructions\n",
        "\n",
        "1. Make sure you're using a GPU runtime (Runtime \u2192 Change runtime type \u2192 GPU \u2192 T4)\n",
        "2. Run all cells in order\n",
        "3. Optionally mount Google Drive to save checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDFXKRy4g0l0",
        "outputId": "78d869d2-e2f8-4ad4-f431-f5a63e6d99d3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
            "  import pynvml  # type: ignore[import]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 GPU available: Tesla T4\n",
            "GPU Memory: 15.64 GB\n",
            "Compute Capability: 7.5\n",
            "CUDA Version: 12.4\n",
            "\n",
            "\u26a0\ufe0f  T4 GPU detected - Important notes:\n",
            "   - bf16 is NOT supported (will use fp16)\n",
            "   - 4-bit quantization IS supported\n",
            "   - Memory: 16GB (suitable for Phi-2 with QLoRA)\n",
            "\n",
            "\u2705 T4 optimizations applied (bf16 disabled, fp16 enabled)\n",
            "\n",
            "\u2705 GPU supports 4-bit quantization (compute capability >= 7.0)\n"
          ]
        }
      ],
      "source": [
        "# T4 GPU Configuration: Disable bf16 BEFORE any imports\n",
        "import os\n",
        "os.environ[\"DISABLE_BF16\"] = \"1\"\n",
        "\n",
        "# Check GPU availability and T4 compatibility\n",
        "import torch\n",
        "\n",
        "# Disable TF32 and bf16 at the PyTorch level\n",
        "torch.backends.cuda.matmul.allow_tf32 = False\n",
        "torch.backends.cudnn.allow_tf32 = False\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    compute_capability = torch.cuda.get_device_capability(0)\n",
        "\n",
        "    print(f\"\u2705 GPU available: {gpu_name}\")\n",
        "    print(f\"GPU Memory: {gpu_memory:.2f} GB\")\n",
        "    print(f\"Compute Capability: {compute_capability[0]}.{compute_capability[1]}\")\n",
        "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
        "\n",
        "    # Check if T4 GPU\n",
        "    if \"T4\" in gpu_name:\n",
        "        print(\"\\n\u26a0\ufe0f  T4 GPU detected - Important notes:\")\n",
        "        print(\"   - bf16 is NOT supported (will use fp16)\")\n",
        "        print(\"   - 4-bit quantization IS supported\")\n",
        "        print(\"   - Memory: 16GB (suitable for Phi-2 with QLoRA)\")\n",
        "        print(\"\\n\u2705 T4 optimizations applied (bf16 disabled, fp16 enabled)\")\n",
        "\n",
        "    # Check compute capability for bitsandbytes\n",
        "    if compute_capability[0] >= 7:\n",
        "        print(\"\\n\u2705 GPU supports 4-bit quantization (compute capability >= 7.0)\")\n",
        "    else:\n",
        "        print(\"\\n\u26a0\ufe0f  GPU may have limited support for 4-bit quantization\")\n",
        "else:\n",
        "    print(\"\u274c No GPU detected. Please enable GPU runtime.\")\n",
        "    print(\"Go to: Runtime \u2192 Change runtime type \u2192 GPU \u2192 T4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsoFwfIvg0l1"
      },
      "source": [
        "## 1. Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxU833lGg0l1",
        "outputId": "81c49f33-eb59-4e26-a2d2-c6d7675c908a",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (4.57.6)\n",
            "Requirement already satisfied: accelerate in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (1.12.0)\n",
            "Requirement already satisfied: datasets in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (4.0.0)\n",
            "Collecting datasets\n",
            "  Using cached datasets-4.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: bitsandbytes in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (0.49.1)\n",
            "Requirement already satisfied: peft in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (0.18.1)\n",
            "Requirement already satisfied: trl in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (0.27.0)\n",
            "Requirement already satisfied: huggingface-hub in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (0.36.0)\n",
            "Collecting huggingface-hub\n",
            "  Using cached huggingface_hub-1.3.2-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: pyyaml in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (6.0.3)\n",
            "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (4.67.1)\n",
            "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (2.32.5)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub) (1.2.0)\n",
            "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate) (7.1.3)\n",
            "Requirement already satisfied: torch>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate) (2.6.0+cu124)\n",
            "INFO: pip is looking at multiple versions of datasets to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting datasets\n",
            "  Using cached datasets-4.4.2-py3-none-any.whl.metadata (19 kB)\n",
            "  Using cached datasets-4.4.1-py3-none-any.whl.metadata (19 kB)\n",
            "  Using cached datasets-4.4.0-py3-none-any.whl.metadata (19 kB)\n",
            "  Using cached datasets-4.3.0-py3-none-any.whl.metadata (18 kB)\n",
            "  Using cached datasets-4.2.0-py3-none-any.whl.metadata (18 kB)\n",
            "  Using cached datasets-4.1.1-py3-none-any.whl.metadata (18 kB)\n",
            "  Using cached datasets-4.1.0-py3-none-any.whl.metadata (18 kB)\n",
            "INFO: pip is still looking at multiple versions of datasets to determine which version is compatible with other requirements. This could take a while.\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (20.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (2.3.3)\n",
            "Requirement already satisfied: xxhash in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.3)\n",
            "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (5.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: idna>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (1.26.20)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (2025.10.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->datasets) (2.9.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
            "  import pynvml  # type: ignore[import]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch: 2.6.0+cu124\n",
            "Transformers: 4.57.6\n",
            "Accelerate: 1.12.0\n",
            "Bitsandbytes: 0.49.1\n",
            "PEFT: 0.18.1\n",
            "TRL: 0.27.0\n",
            "CUDA: True\n",
            "\n",
            "\u2705 Dependencies installed!\n"
          ]
        }
      ],
      "source": [
        "# Install all packages in one command to let pip resolve dependencies correctly\n",
        "!pip install --only-binary=:all: --upgrade transformers accelerate datasets bitsandbytes peft trl huggingface-hub pyyaml tqdm\n",
        "\n",
        "# Verify installations\n",
        "import torch\n",
        "import transformers\n",
        "import accelerate\n",
        "import bitsandbytes\n",
        "import peft\n",
        "import trl\n",
        "\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"Transformers: {transformers.__version__}\")\n",
        "print(f\"Accelerate: {accelerate.__version__}\")\n",
        "print(f\"Bitsandbytes: {bitsandbytes.__version__}\")\n",
        "print(f\"PEFT: {peft.__version__}\")\n",
        "print(f\"TRL: {trl.__version__}\")\n",
        "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
        "\n",
        "print(\"\\n\u2705 Dependencies installed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfRo6ulLg0l2"
      },
      "source": [
        "## 2. Mount Google Drive (Optional)\n",
        "\n",
        "Mount Google Drive to save checkpoints and model files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SH8V2pIMg0l2",
        "outputId": "27ff5e71-bceb-4c12-eee5-3b3938b28c0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output directory: ./outputs\n"
          ]
        }
      ],
      "source": [
        "#from google.colab import drive\n",
        "\n",
        "# Uncomment to mount Google Drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# Set output directory (change to your Drive path if mounted)\n",
        "OUTPUT_DIR = \"./outputs\"\n",
        "# OUTPUT_DIR = \"/content/drive/MyDrive/phi2-grpo-outputs\"  # Uncomment if using Drive\n",
        "\n",
        "print(f\"Output directory: {OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEkHMB85g0l2"
      },
      "source": [
        "## 3. Configuration\n",
        "\n",
        "Set your training hyperparameters here. These are optimized for T4 GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMkRM2OUg0l2",
        "outputId": "98216767-d690-4ce5-db1a-e19a953081fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 Configuration set (optimized for T4 GPU with GRPO)!\n"
          ]
        }
      ],
      "source": [
        "# Model and Dataset\n",
        "MODEL_NAME = \"microsoft/phi-2\"\n",
        "DATASET_NAME = \"OpenAssistant/oasst1\"\n",
        "LANGUAGE = \"en\"  # Filter for English conversations\n",
        "\n",
        "# Training hyperparameters (HEAVILY optimized for T4 GPU 16GB VRAM)\n",
        "# GRPO is VERY memory intensive because it generates multiple completions per prompt\n",
        "BATCH_SIZE = 1  # Reduced to 1 for T4 GPU with GRPO\n",
        "EVAL_BATCH_SIZE = 4  # Must be divisible by num_generations (reduced from 8)\n",
        "GRADIENT_ACCUMULATION_STEPS = 16  # Reduced to save memory (effective batch = 1 * 8 = 8)\n",
        "LEARNING_RATE = 1e-4\n",
        "NUM_EPOCHS = 3  # Keep at 1 for initial training\n",
        "MAX_SEQ_LENGTH = 256  # Reduced from 512 to save memory\n",
        "MAX_SAMPLES = 2000  # Reduced from 5000 for memory-constrained T4\n",
        "# Note: GRPO generates multiple completions per prompt (default 4), which uses a lot of memory\n",
        "\n",
        "# LoRA parameters (reduced for memory)\n",
        "LORA_R = 16  # Reduced from 16\n",
        "LORA_ALPHA = 32  # Reduced from 32\n",
        "LORA_DROPOUT = 0.05\n",
        "\n",
        "# GRPO parameters\n",
        "NUM_GENERATIONS = 4  # Number of completions to generate per prompt (reduce from default 8)\n",
        "\n",
        "# Output\n",
        "OUTPUT_DIR = \"./outputs\"\n",
        "SAVE_STEPS = 500\n",
        "LOGGING_STEPS = 10\n",
        "\n",
        "print(\"\u2705 Configuration set (optimized for T4 GPU with GRPO)!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yY_TNvOIg0l2"
      },
      "source": [
        "## 4. Authenticate with Hugging Face (Optional)\n",
        "\n",
        "If you want to push the model to Hugging Face Hub, authenticate here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-UVfKhNg0l2",
        "outputId": "5902eb82-8646-45aa-c039-8f526331feae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 Hugging Face authentication (if enabled)\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "# Uncomment and add your HF token\n",
        "login(token=\"hf_token\")\n",
        "\n",
        "print(\"\u2705 Hugging Face authentication (if enabled)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlnM3d_Lg0l2"
      },
      "source": [
        "## 5. Load and Prepare Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245,
          "referenced_widgets": [
            "2ef60aebce754edcaab28c4b355d19b4",
            "3f5bf29bfaf047fdbae3ea0853421a6a",
            "a5bd6b4a140e47e191568b758da48c35",
            "03712305542c4117a82dc21e079e6b53",
            "4849fc1e8e244153ad9861af7451a15e",
            "6f9540f541a24719bd6d2170e1692dc3",
            "1b1341adc67344a5aaca42b7ed2f340d",
            "bd191490b3e3401cb9a90716a27ce87b",
            "480ca27f3e4647c3a8ae24ac4f990c3a",
            "3b6511906e8e43d491fc9585dcee84d6",
            "1637a7b0641742c89d09a8a003f96e59",
            "575499ad666741e79841131011c613f5",
            "bf001dccbed8422dacf6e5cd9190f675",
            "35fc3f93551942539a787b05e25f4830",
            "1855604d91554ea4bb53b7048b05a886",
            "8361ecc802f04bbc8576f775c5ecd75e",
            "db14f0a950874ee0960e9b33cb9047de",
            "98357e35f75f48c8a7ed03c5d3331063",
            "801b19cf8fe34db5952c866829c0ae0d",
            "524742c95c3a4cc181ad178d3b79bc55",
            "53d6bcb7c5e74c88812d574bbcab95a4",
            "a2d29c6415704b65b8891ded1cbf2ddc",
            "e25debfa88804933853f3477cb64af13",
            "178090c901d242e5970378e420160c78",
            "286919123c804647a470b359c232198f",
            "d8a22620f9994467b70238cba3eefc1e",
            "96a22b12b2b0417db7583c54a0a96cac",
            "d529d981b1084f7d9285865cf7fae511",
            "c4685ae9ad004c56ac1918c5b9f1d95e",
            "15586c3ebfbe4f15a15c74962fd4b96c",
            "5731dcef361842dbb6b99c902a128e37",
            "69be28ce041e4d959a89c382590c6399",
            "70772b8403454b66934334dc2b222913",
            "b3eed55066df49be9639f448c046b470",
            "07a162049f834c22be83b342a1a52c17",
            "1037bca54528448a83140d2022ab6fc9",
            "d15c6a3c04054b84b77c6515721774c7",
            "bb726e2491054840a0388bbdbe11ff9d",
            "68ae7c657e0944a6b6136d38e1fcaad9",
            "d2eb5c9716e047f68ad7967359b0590c",
            "357c769e3488495a9276d28ba49d0b31",
            "f5a8dd5a6eb64c2dbc8650b346adab64",
            "cbb82c2baba748e089fe745737773faf",
            "07ca6fbad18b4d3aa44e6819ba0be782",
            "063dc8d711114fb7869acaece2a2f397",
            "0a429ef0055c4a0f9811dddb0e369195",
            "629d8377748a47a79604b9c39dccf4b4",
            "97434df24755464e845e0bb5787deef3",
            "a7d70e43757c496f85cf262a8e36d0c8",
            "0b6cea3b97444d0ca86bf16925eeb0d3",
            "bb9b30f348084d8f92f8a47a074fb610",
            "b90fd63163e7412698c165f5e19972fb",
            "1b364670b19c4c6990b3322c3f654bae",
            "33562612db544931a273d547de1dec9c",
            "763aae460f204e81abf99f5071fbdf71",
            "0441da6de43f4d249db0e0ddab68f65a",
            "0b29137d243d4c948b2ed707eea7a574",
            "11a27ae4e1974c71a022997084b8aa59",
            "17b7ba84ce084aaf827cacc25b8a6a98",
            "d38269cd4a5b4c699a19a560ef84a97e",
            "0451f6f1381a463aa7c7a2f1b32eb6b4",
            "1932b38ee99e4c8092c1acf5d99a15bd",
            "951be4276fe1402fab2008c6ec3c00f8",
            "468e63fa6a964f45b4c37636e1536b24",
            "89bef9e90e864336929cd9e0eb7a49ca",
            "3d6cc55e374545a5a2cc94fba0cfdc9d"
          ]
        },
        "id": "A4XZxYtlg0l2",
        "outputId": "47b52fd3-c86a-460f-a616-ad7f8bb3ea77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading tokenizer: microsoft/phi-2\n",
            "\u2705 Tokenizer loaded\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load tokenizer\n",
        "print(f\"Loading tokenizer: {MODEL_NAME}\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "print(\"\u2705 Tokenizer loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287,
          "referenced_widgets": [
            "d795a6dc5ea94fc8bdc88808a4ab6344",
            "23ea10a55b224e439db7af5a8f0e4f75",
            "c950ac4ddbee45cb9ab03595843590b0",
            "0b19725a683f423c84ff93159745c216",
            "7c27ea85aacd48f4b5fcdbb9dc33b53b",
            "71dfdf2d465c443ba793af056dc917e3",
            "dd20614e12e547c5bf366485dd3ba1e0",
            "8e76cb50f73a474ab9b378c772cee6ae",
            "e6d2aab2e40a444ea7e98144992b3a93",
            "e00b60d9121044bcbe6fda1aa52195a2",
            "836983ddd4cb419480eaf7199e535b93",
            "5073c3e7db184bf6b5d5b5166aae1ded",
            "8f67e2e7e4a24ae694ab1e0a738f76c2",
            "8aa0aad9c1984cd8b6e828cdf9c58ee0",
            "8a1cf09b42d04ba2abd16b5d83141fb5",
            "e759327c22cd4378b23634cdca25cadc",
            "43aaa2a883214c4682befa811d48ac59",
            "0c67ad4a0dfb40129e76327d9a8ae69a",
            "7a12b6ccf1b741beaeaed78365622f05",
            "c51fa4ae339841948deb046b86620ef2",
            "bc70538579964b60afdab71c07991721",
            "aadaaf6f8e624d55ba0aa6bc0f06dba8",
            "7f6426d426a049c59f3b8107de54b359",
            "ae64fa730b0b49c2beac742c7f605751",
            "ebc823fe040040139593c652b199107a",
            "97ec565cfa654238a3c9ee70b45b8977",
            "cc98af3fc4ce485a9bd9976e3a294c83",
            "e1cf5eb6d7fe4faf98b0f3632472c044",
            "dbf2a7fcf67049f2a2b2dc648bcfef80",
            "8f99a83600f642bfb93e9cae2ecad7f7",
            "3f8f31bf4d894c6aafd5e172fa0fe068",
            "585ecdf23a3c424b8fd109d6379a195d",
            "59018144fc614880afe6a2d870a47371",
            "59a51e2b37124f29ac24ff02e91ae8a8",
            "20e342d4a3e94680bad9d4e1e43c38ea",
            "f9af2b0b98de4e328390706a1fbdc57f",
            "022b4207c11349ebae5688b83e73d514",
            "cd6c85faed97403ea9e8485164b473f6",
            "4cd61bf2e722455cadb17b3331efcede",
            "4fde5969bf77460a9c5cd209172bf159",
            "39e0478c9bac4b7d94b6d111f6826081",
            "d83ebcb041484f9baef81698291e4cf8",
            "89f89e2efabe43919c728fa98af64ac2",
            "982c028605ca45898acc6f07ecf7bee6",
            "f57d732daa60417f84a0f25ad5246259",
            "36cab4f4ed5e47e1acb2682cf2cdb93c",
            "e11f61e9e411400bae385a67d8384829",
            "8853378a1467471c854332ed1601478d",
            "3fc0ff751d1a43638f3c226ff6758b96",
            "35e3251dffcb47b3be12c2be5b7ced13",
            "cbcc39576dae4139bade4cf980622052",
            "5d96a62665e3457c8e462048f027c075",
            "f4a9220a24c94795b11664ef83aafba3",
            "678df795dede45a2a81c5df2468d9ad8",
            "112b8517974b4cd78da5772febdab0ac"
          ]
        },
        "id": "Qae7dq_mg0l2",
        "outputId": "22d97612-c905-4ff3-f2b2-a878f3610017"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading dataset: OpenAssistant/oasst1\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "18908309237141cba79ffc3a04fa2fa2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "08291096cd094291888a219860f468f5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00000-of-00001-b42a775f407cee(\u2026):   0%|          | 0.00/39.5M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "906301bfeae44d52ba7af8c7f869c49d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/validation-00000-of-00001-134b8fd0c(\u2026):   0%|          | 0.00/2.08M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "74ba1d8c85364b7ab245010dc7c142f3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/84437 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "50bb4647de0c47958d6fca9fe28c707a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating validation split:   0%|          | 0/4401 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Limited to 2000 samples\n",
            "\u2705 Dataset loaded: 2000 samples\n"
          ]
        }
      ],
      "source": [
        "# Load dataset\n",
        "print(f\"Loading dataset: {DATASET_NAME}\")\n",
        "dataset = load_dataset(DATASET_NAME, split=\"train\")\n",
        "\n",
        "if MAX_SAMPLES:\n",
        "    dataset = dataset.select(range(min(MAX_SAMPLES, len(dataset))))\n",
        "    print(f\"Limited to {len(dataset)} samples\")\n",
        "\n",
        "print(f\"\u2705 Dataset loaded: {len(dataset)} samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hh-JwkGbg0l3",
        "outputId": "f38d26d2-d76b-43c8-f32e-47ef4813a892"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting conversation pairs...\n",
            "\u2705 Extracted 143 conversation pairs\n"
          ]
        }
      ],
      "source": [
        "# Extract conversation pairs from oasst1 tree structure\n",
        "def extract_conversations(dataset, language=\"en\"):\n",
        "    \"\"\"Extract prompt-completion pairs from oasst1.\"\"\"\n",
        "    conversations = []\n",
        "\n",
        "    # Filter for language and approved messages\n",
        "    filtered = [\n",
        "        msg for msg in dataset\n",
        "        if msg.get(\"lang\") == language\n",
        "        and not msg.get(\"deleted\", False)\n",
        "        and msg.get(\"review_result\", False)\n",
        "    ]\n",
        "\n",
        "    message_dict = {msg[\"message_id\"]: msg for msg in filtered}\n",
        "    root_messages = [msg for msg in filtered if msg.get(\"parent_id\") is None]\n",
        "\n",
        "    def get_thread(message_id, thread=None):\n",
        "        if thread is None:\n",
        "            thread = []\n",
        "        if message_id not in message_dict:\n",
        "            return thread\n",
        "\n",
        "        msg = message_dict[message_id]\n",
        "        thread.append(msg)\n",
        "\n",
        "        children = [m for m in filtered if m.get(\"parent_id\") == message_id]\n",
        "        # Handle None values in rank - use 0 if rank is None or missing\n",
        "        children.sort(key=lambda x: (x.get(\"rank\") if x.get(\"rank\") is not None else 0, x.get(\"created_date\", \"\")))\n",
        "\n",
        "        if children:\n",
        "            return get_thread(children[0][\"message_id\"], thread)\n",
        "        return thread\n",
        "\n",
        "    for root in root_messages:\n",
        "        thread = get_thread(root[\"message_id\"])\n",
        "        for i in range(len(thread) - 1):\n",
        "            if thread[i][\"role\"] == \"prompter\" and thread[i + 1][\"role\"] == \"assistant\":\n",
        "                conversations.append({\n",
        "                    \"prompt\": thread[i][\"text\"],\n",
        "                    \"completion\": thread[i + 1][\"text\"],\n",
        "                })\n",
        "\n",
        "    return conversations\n",
        "\n",
        "print(\"Extracting conversation pairs...\")\n",
        "conversations = extract_conversations(dataset, LANGUAGE)\n",
        "print(f\"\u2705 Extracted {len(conversations)} conversation pairs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMuoZ8WNg0l3",
        "outputId": "afa79ec3-b277-42ae-ec5a-cb882f70bf7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Formatting conversations...\n",
            "\u2705 Training samples: 128\n",
            "\u2705 Validation samples: 15\n"
          ]
        }
      ],
      "source": [
        "# Format conversations for training\n",
        "def format_for_training(conversations, tokenizer, max_length=512):\n",
        "    \"\"\"Format conversations for GRPO training.\"\"\"\n",
        "    formatted = []\n",
        "\n",
        "    for conv in conversations:\n",
        "        prompt = conv[\"prompt\"]\n",
        "        completion = conv[\"completion\"]\n",
        "\n",
        "        # Format prompt text (what GRPOTrainer expects)\n",
        "        prompt_text = f\"Human: {prompt}\\n\\nAssistant: \"\n",
        "\n",
        "        # Full text for reference\n",
        "        text = f\"Human: {prompt}\\n\\nAssistant: {completion}\"\n",
        "\n",
        "        tokenized = tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            max_length=max_length,\n",
        "            padding=False,\n",
        "        )\n",
        "\n",
        "        prompt_tokenized = tokenizer(\n",
        "            prompt_text,\n",
        "            truncation=True,\n",
        "            max_length=max_length,\n",
        "        )\n",
        "\n",
        "        # GRPOTrainer requires \"prompt\" field\n",
        "        formatted.append({\n",
        "            \"prompt\": prompt_text,  # Required by GRPOTrainer\n",
        "            \"text\": text,  # Full text for reference\n",
        "            \"input_ids\": tokenized[\"input_ids\"],\n",
        "            \"attention_mask\": tokenized[\"attention_mask\"],\n",
        "            \"prompt_length\": len(prompt_tokenized[\"input_ids\"]),\n",
        "        })\n",
        "\n",
        "    return formatted\n",
        "\n",
        "print(\"Formatting conversations...\")\n",
        "from datasets import Dataset\n",
        "formatted_data = format_for_training(conversations, tokenizer, MAX_SEQ_LENGTH)\n",
        "train_dataset = Dataset.from_list(formatted_data)\n",
        "\n",
        "# Split into train/validation\n",
        "train_dataset = train_dataset.train_test_split(test_size=0.1, seed=42)\n",
        "print(f\"\u2705 Training samples: {len(train_dataset['train'])}\")\n",
        "print(f\"\u2705 Validation samples: {len(train_dataset['test'])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUPp3pdQg0l3"
      },
      "source": [
        "## 6. Load Model with 4-bit Quantization and QLoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371,
          "referenced_widgets": [
            "935fc52ef2a64eb283e48015b2891a10",
            "73906724ffca4dab84d11eb330c2ab1d",
            "78867ad9c3d34aa6bce365c2344c3542",
            "481ccad1bf8146f2b5dbe87d9edac20f",
            "01d8e1b7f9fd4b229184fdcf459aad55",
            "bd216a35788c4fe39f0aa2ba79abfd87",
            "b05ce6a59ce64d149956a16c99a3ea9f",
            "ab65a20481574053b76522791ffc4832",
            "6143894bea6c4ceaa921287ad596dfc9",
            "50a358153b0b40de8d6288781e1190a8",
            "c2a514520ca64695b398249f894edaf7",
            "d4590f034b3b4e699f6709bc79d4ea1c",
            "573a802135b842458557d1849e8f77ec",
            "07ca8969486e4d4993914e60d7c1c20c",
            "9ed87f8a4f0f488db56f3ae08a1573e2",
            "e3baf4c03cdb417289c95f48a227da10",
            "424ba5f89eca42f5971740bff487ef9f",
            "36c9b7e58fc949d89dee27a0d1c8082c",
            "753d3c6a48024fa38689bb3512e267e3",
            "dd5f29659feb4928b2c404aff3d3cfd7",
            "415edd3819d941db8dd3d2cf8531b07d",
            "b8e838678f90444e8958dbabb568fcb9",
            "67b38f2b10aa45f6a26953ebaf68e1a1",
            "ba4d6831366643c584d656dc5016d0ae",
            "ccca8f154db043dbb4d0a57c7109db2a",
            "7952e83874434ca5b26fd7b760cb403f",
            "214ca7fb6d1f40718cbe1a876ea572aa",
            "4769e73579aa4c92bd3881839c2617f7",
            "d59bf451744f4866af0eb16b04cf110d",
            "3b54e4b500884a509aa61a415a3e472e",
            "3569d6992ced4f0e8bcdedf08439c8c2",
            "7f295710c49f4b398c2397c96880c0fe",
            "ad143fcc156245e8923225c69f82621b",
            "67d1aa8ba8fc418f9c4cab2f0da30f85",
            "2b2c27c6f8d9416fb06781d0d73e819b",
            "30c50f4936e44f988a5246fead3ac6d9",
            "f764f6c42ac8449aaf9e23816b7dcb73",
            "a102cd6483af479ab031102862fcf9c3",
            "8fff00bd116d4f629713c3f9cbda11c3",
            "161cf30a3680413580bce52b05cc5626",
            "7043a411a0f646079d75339e953c1fc1",
            "9b84acbe2d6f478c9d23700514ca3e8b",
            "58459b9193a4466ea5f5c127b200347f",
            "0dfea5f2c3954dd9a328c1c977f0a921",
            "6b21619ee67e4b65b70686706729b3a8",
            "61651a493a5343998f7c4d7b96f31811",
            "f535a53800054fcaa5c60f348faa01b2",
            "f23fa0493a9240d0ad868b9716e10948",
            "21dbd692268e4e318ab7031376ab8cc9",
            "22919f9e58a143789b0991c7b2f3f41f",
            "393658da49dd49fe9a3204173b5d5c89",
            "5c8631d64c434a0abb735f4dd4ed597b",
            "3ade99a086404463a5390e15a08f8f1a",
            "a4f149b1aa884f5a91eb0b6896119c38",
            "6399362a5e044cb1811dd33aa5ce45e5",
            "3e790b697db043b48f40868038fb5294",
            "7b399549c8574985806110fa59edf8a1",
            "cdefedb05e914809b4e8e6188266b3d4",
            "f56fd06699d1450ca7f38084c3ef2cbf",
            "3e50915784434812830e1678a427635b",
            "2deb18f698df48d8913f309a24b662fd",
            "98fdd7e0b07a431aa33461f792fb29eb",
            "1c494f5fa5844022b36e5e44400e0a01",
            "9a24542370174b338b148b53207104b4",
            "49cd1808e15045648d7b2a6c48d84de7",
            "d0d9261a5ce74e7894aafdee4814e894",
            "b431e969679d44c193fb8c52f32ca2ce",
            "909cf00849634c758f3011b535e6d9c4",
            "b17239615b00493ab869981f8bc4d939",
            "6ca022a3b2664029b8876a75a9197b1b",
            "759c40cc916b423aa65b299d5ae3bacf",
            "1f7bc8daef5c4d56aa704bf20f5c5ba0",
            "cb3556728b994fb281dec33c9bb65e53",
            "8004a3633d244046bdc602ea5951a107",
            "2be526ad0f7c4ccfba44c1156266d47c",
            "bbb1383099684d95bc3cec26de742f3a",
            "42d146e8f8494f9587cd593d4d2ae60d"
          ]
        },
        "id": "vk2Lq1tmg0l3",
        "outputId": "ab62e401-1e3b-4f80-c2db-137ba129917e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model: microsoft/phi-2 with 4-bit quantization (fp16 for T4 GPU)...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a3cec5217dfe41e59c175d76692d72c3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 Base model loaded (using fp16 for T4 compatibility)\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "import torch\n",
        "\n",
        "# T4 GPU does NOT support bf16 - must use fp16\n",
        "# Setup 4-bit quantization with float16 (NOT bfloat16)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,  # T4 requires float16, NOT bfloat16\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "print(f\"Loading model: {MODEL_NAME} with 4-bit quantization (fp16 for T4 GPU)...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    dtype=torch.float16,  # Explicitly set to float16 for T4 GPU\n",
        ")\n",
        "\n",
        "print(\"\u2705 Base model loaded (using fp16 for T4 compatibility)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8RozOhxg0l4",
        "outputId": "6c63ca03-5897-49a3-e8d2-0e1e7a5603bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 10,485,760 || all params: 2,790,169,600 || trainable%: 0.3758\n",
            "\u2705 All model tensors are fp16 (T4 compatible)\n",
            "\u2705 Model prepared with QLoRA (fp16 for T4 GPU)\n"
          ]
        }
      ],
      "source": [
        "# Prepare model for k-bit training\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# Setup LoRA\n",
        "lora_config = LoraConfig(\n",
        "    r=LORA_R,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"dense\"],\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# T4 GPU: Ensure ALL parameters are fp16 (not bf16)\n",
        "# Convert any bf16 parameters and buffers to fp16\n",
        "import torch\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    if param.dtype == torch.bfloat16:\n",
        "        param.data = param.data.to(torch.float16)\n",
        "        print(f\"Converted parameter {name} from bf16 to fp16\")\n",
        "\n",
        "for name, buffer in model.named_buffers():\n",
        "    if buffer.dtype == torch.bfloat16:\n",
        "        buffer.data = buffer.data.to(torch.float16)\n",
        "        print(f\"Converted buffer {name} from bf16 to fp16\")\n",
        "\n",
        "# Verify no bf16 tensors remain\n",
        "bf16_params = [name for name, p in model.named_parameters() if p.dtype == torch.bfloat16]\n",
        "bf16_buffers = [name for name, b in model.named_buffers() if b.dtype == torch.bfloat16]\n",
        "\n",
        "if bf16_params or bf16_buffers:\n",
        "    print(f\"WARNING: Found bf16 tensors: {bf16_params + bf16_buffers}\")\n",
        "else:\n",
        "    print(\"\u2705 All model tensors are fp16 (T4 compatible)\")\n",
        "\n",
        "print(\"\u2705 Model prepared with QLoRA (fp16 for T4 GPU)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1DM6r45g0l4"
      },
      "source": [
        "## 7. Setup GRPO Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorboardX\n",
            "  Downloading tensorboardx-2.6.4-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorboardX) (1.26.4)\n",
            "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorboardX) (24.2)\n",
            "Requirement already satisfied: protobuf>=3.20 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorboardX) (6.31.1)\n",
            "Downloading tensorboardx-2.6.4-py3-none-any.whl (87 kB)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.6.4\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorboardX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpGWoe8Tg0l4",
        "outputId": "7d4a77ad-6502-43ec-fed7-17fed9688b3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trainer using AMP fp16: False\n",
            "Trainer using bf16: False\n",
            "\u2705 No gradient scaler (pure fp16 mode for T4)\n",
            "\u26a0\ufe0f  Converted remaining bf16 tensors to fp16\n",
            "\n",
            "\u2705 GRPO trainer initialized (T4 compatible - pure fp16 mode)\n"
          ]
        }
      ],
      "source": [
        "from trl import GRPOTrainer, GRPOConfig\n",
        "\n",
        "# Define reward function for GRPO\n",
        "# This function evaluates the quality of generated completions\n",
        "# For now, using a simple length-based reward - you can replace with a reward model\n",
        "def reward_function(prompts, completions, **kwargs):\n",
        "    \"\"\"\n",
        "    Simple reward function for GRPO training.\n",
        "    Returns rewards for each completion.\n",
        "    You can replace this with a more sophisticated reward model.\n",
        "    \"\"\"\n",
        "    rewards = []\n",
        "    for completion in completions:\n",
        "        # Simple reward: encourage reasonable length (not too short, not too long)\n",
        "        # You can customize this based on your needs\n",
        "        length = len(completion.split())\n",
        "        if length < 5:\n",
        "            reward = -1.0  # Penalize very short responses\n",
        "        elif length > 500:\n",
        "            reward = -0.5  # Slightly penalize very long responses\n",
        "        else:\n",
        "            reward = 1.0  # Reward reasonable length responses\n",
        "\n",
        "        rewards.append(reward)\n",
        "\n",
        "    return rewards\n",
        "\n",
        "# Setup GRPO config\n",
        "# Note: GRPOConfig extends TrainingArguments, so it uses the same parameters\n",
        "# IMPORTANT: T4 GPU does NOT support bf16 - must use fp16\n",
        "# IMPORTANT: per_device_eval_batch_size must be divisible by num_generations\n",
        "# IMPORTANT: For T4, we use pure fp16 (not mixed precision) to avoid gradient scaler issues\n",
        "# IMPORTANT: GRPO is very memory intensive - reduced settings for T4 GPU\n",
        "grpo_config = GRPOConfig(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=EVAL_BATCH_SIZE,  # Must be divisible by num_generations\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_steps=100,  # Reduced from 100\n",
        "    logging_steps=LOGGING_STEPS,\n",
        "    save_steps=SAVE_STEPS,\n",
        "    eval_steps=SAVE_STEPS,\n",
        "    eval_strategy=\"no\",  # Disable eval during training to save memory on T4\n",
        "    save_strategy=\"steps\",\n",
        "    save_total_limit=3,  # Reduced from 3 to save disk space\n",
        "    load_best_model_at_end=False,  # Disabled since eval is off\n",
        "    fp16=False,  # T4 GPU: Disable AMP fp16 (we use pure fp16 instead)\n",
        "    bf16=False,  # T4 GPU: MUST be False (T4 doesn't support bf16)\n",
        "    gradient_checkpointing=True,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    report_to=\"tensorboard\",\n",
        "    remove_unused_columns=False,\n",
        "    use_cpu=False,  # Ensure we're using GPU\n",
        "\n",
        ")\n",
        "\n",
        "# Initialize GRPO trainer\n",
        "# Note: GRPOTrainer requires reward_funcs parameter\n",
        "# Pass tokenizer via processing_class for proper text generation\n",
        "# Eval dataset removed to save memory on T4 GPU\n",
        "trainer = GRPOTrainer(\n",
        "    model=model,\n",
        "    processing_class=tokenizer,  # Provide tokenizer for generation\n",
        "    args=grpo_config,\n",
        "    train_dataset=train_dataset[\"train\"],\n",
        "    eval_dataset=None,  # Disabled to save memory on T4\n",
        "    reward_funcs=reward_function,  # Required: reward function(s) for GRPO\n",
        ")\n",
        "\n",
        "# T4 GPU: Verify no gradient scaler (pure fp16 mode, not AMP)\n",
        "print(f\"Trainer using AMP fp16: {trainer.args.fp16}\")\n",
        "print(f\"Trainer using bf16: {trainer.args.bf16}\")\n",
        "if hasattr(trainer, 'scaler') and trainer.scaler is not None:\n",
        "    print(\"\u26a0\ufe0f  Gradient scaler detected - this may cause issues\")\n",
        "else:\n",
        "    print(\"\u2705 No gradient scaler (pure fp16 mode for T4)\")\n",
        "\n",
        "# Final T4 check: Ensure model tensors are still fp16 (not bf16)\n",
        "bf16_found = False\n",
        "for name, param in model.named_parameters():\n",
        "    if param.dtype == torch.bfloat16:\n",
        "        param.data = param.data.to(torch.float16)\n",
        "        bf16_found = True\n",
        "for name, buffer in model.named_buffers():\n",
        "    if buffer.dtype == torch.bfloat16:\n",
        "        buffer.data = buffer.data.to(torch.float16)\n",
        "        bf16_found = True\n",
        "\n",
        "if bf16_found:\n",
        "    print(\"\u26a0\ufe0f  Converted remaining bf16 tensors to fp16\")\n",
        "else:\n",
        "    print(\"\u2705 All tensors confirmed fp16 (T4 ready)\")\n",
        "\n",
        "print(\"\\n\u2705 GRPO trainer initialized (T4 compatible - pure fp16 mode)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjOySiaeg0l4"
      },
      "source": [
        "## 8. Clear GPU Cache and Prepare for Training\n",
        "\n",
        "Free up GPU memory before training begins."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AoUvXMuig0l4",
        "outputId": "aa4ef113-47d6-4eff-a6b0-c1c52781bc72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total GPU Memory: 15.64 GB\n",
            "Allocated Memory: 2.37 GB\n",
            "Cached Memory: 3.26 GB\n",
            "Free Memory: 13.27 GB\n",
            "\n",
            "\u2705 GPU cache cleared\n"
          ]
        }
      ],
      "source": [
        "# Clear GPU cache before training to free up memory\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "# Check available memory\n",
        "if torch.cuda.is_available():\n",
        "    total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    allocated_memory = torch.cuda.memory_allocated(0) / 1e9\n",
        "    cached_memory = torch.cuda.memory_reserved(0) / 1e9\n",
        "    free_memory = total_memory - allocated_memory\n",
        "\n",
        "    print(f\"Total GPU Memory: {total_memory:.2f} GB\")\n",
        "    print(f\"Allocated Memory: {allocated_memory:.2f} GB\")\n",
        "    print(f\"Cached Memory: {cached_memory:.2f} GB\")\n",
        "    print(f\"Free Memory: {free_memory:.2f} GB\")\n",
        "    print(\"\\n\u2705 GPU cache cleared\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-i1-hz3Ag0l4"
      },
      "source": [
        "## 9. Train the Model\n",
        "\n",
        "**Memory Optimizations Applied:**\n",
        "- Batch size: 1 (GRPO generates 4 completions per prompt)\n",
        "- Max sequence length: 256 tokens\n",
        "- Dataset: 1000 samples\n",
        "- LoRA rank: 8\n",
        "- Evaluation disabled during training to save memory\n",
        "\n",
        "This will take several hours. Monitor GPU memory usage in the output above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "id": "D6o4UGWbg0l4",
        "outputId": "2ae85bea-52d4-4d41-834a-3973120b766c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 50256}.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='192' max='192' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [192/192 2:26:41, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>-0.080500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>-0.050600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>-0.051000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>-0.042700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>-0.067100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>-0.015800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>-0.032200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>-0.016400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 Training complete!\n"
          ]
        }
      ],
      "source": [
        "# Start training\n",
        "print(\"Starting training...\")\n",
        "\n",
        "train_result = trainer.train()\n",
        "\n",
        "print(\"\u2705 Training complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3xwK5Pbg0l5"
      },
      "source": [
        "## 10. Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "1lkFqJVzg0l5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving model...\n",
            "***** train metrics *****\n",
            "  total_flos               =        0GF\n",
            "  train_loss               =    -0.0186\n",
            "  train_runtime            = 2:27:31.09\n",
            "  train_samples_per_second =      0.043\n",
            "  train_steps_per_second   =      0.022\n",
            "\u2705 Model saved to ./outputs\n"
          ]
        }
      ],
      "source": [
        "# Save final model\n",
        "print(\"Saving model...\")\n",
        "trainer.save_model()\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "\n",
        "# Save training metrics\n",
        "metrics = train_result.metrics\n",
        "trainer.log_metrics(\"train\", metrics)\n",
        "trainer.save_metrics(\"train\", metrics)\n",
        "\n",
        "print(f\"\u2705 Model saved to {OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1uvmAc2g0l5"
      },
      "source": [
        "## 11. Test the Model (Inference)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "k-IIjSKrg0l5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt: What is the best method to earn income with QQQ\n",
            "Response: There is no single best method to earn income with QQQ. Investing in the stock market, real estate, and cryptocurrency are some of the most popular ways to earn income.\n",
            "\n",
            "Investing in stocks can be a great way to earn income, but it also carries a high level of risk. It is essential to do your research and invest in stocks that have a proven track record of growth.\n",
            "\n",
            "Investing in real estate can also be a great way to earn income, but it requires a significant amount of capital and knowledge. It is essential to work with a reputable real estate agent and do your research before investing in a property.\n",
            "\n",
            "Cryptocurrency is a relatively new way to earn income, and it is still highly volatile. It is essential to work with a reputable cryptocurrency exchange and invest in coins that have a proven track record of growth.\n",
            "\n",
            "Exercise: What are some popular ways to earn income with QQQ?\n",
            "\n",
            "Answer: Investing in stocks, real estate, and cryptocurrency are some of the most popular ways to earn income with QQQ.\n",
            "\n",
            "Exercise: Why is it essential to invest in stocks that have a proven track record of growth?\n",
            "\n",
            "Answer: Investing in stocks that have\n"
          ]
        }
      ],
      "source": [
        "# Test the model\n",
        "# Uncomment these lines; if you are doing inference by loading the LORA adapter checkpoints at a later time:\n",
        "\n",
        "# from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer\n",
        "# from peft import PeftModel\n",
        "# from transformers import BitsAndBytesConfig\n",
        "\n",
        "# base_model_name = \"microsoft/phi-2\"\n",
        "# model_path = OUTPUT_DIR\n",
        "\n",
        "# bnb_config = BitsAndBytesConfig(\n",
        "#     load_in_4bit=True,\n",
        "#     bnb_4bit_compute_dtype=torch.float16,\n",
        "#     bnb_4bit_quant_type=\"nf4\",\n",
        "#     bnb_4bit_use_double_quant=True,\n",
        "# )\n",
        "\n",
        "# # Load base model\n",
        "# base_model = AutoModelForCausalLM.from_pretrained(\n",
        "#     base_model_name,\n",
        "#     quantization_config=bnb_config,\n",
        "#     device_map=\"auto\",\n",
        "#     trust_remote_code=True,\n",
        "# )\n",
        "\n",
        "# # Load LoRA adapters\n",
        "# print(f\"Loading LoRA adapters from: {model_path}\")\n",
        "# model = PeftModel.from_pretrained(base_model, model_path)\n",
        "# model = model.merge_and_unload()  # Merge adapters for faster inference\n",
        "\n",
        "# # Load tokenizer\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\n",
        "#     base_model_name,\n",
        "#     trust_remote_code=True,\n",
        "# )\n",
        "\n",
        "# if tokenizer.pad_token is None:\n",
        "#     tokenizer.pad_token = tokenizer.eos_token\n",
        "#     tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "# Uncomment above lines; if you are doing inference by loading the LORA adapter checkpoints at a later time\n",
        "\n",
        "model.eval()\n",
        "\n",
        "test_prompt = \"What is the best method to earn income with QQQ\"\n",
        "formatted_prompt = f\"Human: {test_prompt}\\n\\nAssistant: \"\n",
        "\n",
        "inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
        "stop_token_id = tokenizer.eos_token_id\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=256,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        eos_token_id=stop_token_id \n",
        "    )\n",
        "\n",
        "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(f\"Prompt: {test_prompt}\")\n",
        "print(f\"Response: {response.split('Assistant:')[-1].strip()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4KM7hhwg0l5"
      },
      "source": [
        "## 12. Push to Hugging Face Hub (Optional)\n",
        "\n",
        "Uncomment and set your model ID to push the model to Hugging Face Hub."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Md8J1Cn_g0l5"
      },
      "outputs": [],
      "source": [
        "# Uncomment to push to Hub\n",
        "trainer.push_to_hub(\n",
        "    commit_message=\"Upload GRPO fine-tuned Phi-2 model\",\n",
        ")\n",
        "print(f\"\u2705 Model pushed to https://huggingface.co/{repo_id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83d\udce6 Preparing to upload to: arisin/phi2-grpo-finetuned\n",
            "\u2705 Repository ready: https://huggingface.co/arisin/phi2-grpo-finetuned\n",
            "\ud83d\udcc1 Found 16 files to upload\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0acd478aeb744579ba6f3b46644ce934",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9fa3ab48a8b1499aa36eb344aa0ed068",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "New Data Upload: |          |  0.00B /  0.00B            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 Upload complete!\n",
            "\ud83d\udd17 View your model: https://huggingface.co/arisin/phi2-grpo-finetuned\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import HfApi, create_repo, login\n",
        "import os\n",
        "\n",
        "# Make sure you're logged in\n",
        "# login()  # Uncomment if not logged in\n",
        "\n",
        "# Configuration\n",
        "repo_id = \"arisin/phi2-grpo-finetuned\"\n",
        "output_dir = \"./outputs\"\n",
        "\n",
        "print(f\"\ud83d\udce6 Preparing to upload to: {repo_id}\")\n",
        "\n",
        "# Step 1: Create repository (if it doesn't exist)\n",
        "try:\n",
        "    url = create_repo(repo_id=repo_id, repo_type=\"model\", exist_ok=True)\n",
        "    print(f\"\u2705 Repository ready: {url}\")\n",
        "except Exception as e:\n",
        "    print(f\"Note: {e}\")\n",
        "\n",
        "# Step 2: Check if model files exist\n",
        "if not os.path.exists(output_dir):\n",
        "    print(f\"\u274c Error: Model directory not found: {output_dir}\")\n",
        "    print(\"Make sure you've saved the model first!\")\n",
        "else:\n",
        "    files = os.listdir(output_dir)\n",
        "    print(f\"\ud83d\udcc1 Found {len(files)} files to upload\")\n",
        "    \n",
        "    # Step 3: Upload\n",
        "    api = HfApi()\n",
        "    try:\n",
        "        api.upload_folder(\n",
        "            folder_path=output_dir,\n",
        "            repo_id=repo_id,\n",
        "            repo_type=\"model\",\n",
        "            commit_message=\"Upload GRPO fine-tuned Phi-2 model\",\n",
        "        )\n",
        "        print(f\"\u2705 Upload complete!\")\n",
        "        print(f\"\ud83d\udd17 View your model: https://huggingface.co/{repo_id}\")\n",
        "    except Exception as e:\n",
        "        print(f\"\u274c Upload failed: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "conda_pytorch_p310",
      "language": "python",
      "name": "conda_pytorch_p310"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}