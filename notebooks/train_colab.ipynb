{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Phi-2 GRPO Fine-tuning on Google Colab\n",
        "\n",
        "This notebook fine-tunes the Microsoft Phi-2 model using GRPO (Group Relative Policy Optimization) on the OpenAssistant/oasst1 dataset with QLoRA 4-bit quantization.\n",
        "\n",
        "**Optimized for T4 GPU (16GB VRAM)**\n",
        "\n",
        "## Setup Instructions\n",
        "\n",
        "1. Make sure you're using a GPU runtime (Runtime → Change runtime type → GPU → T4)\n",
        "2. Run all cells in order\n",
        "3. Optionally mount Google Drive to save checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# T4 GPU Configuration: Disable bf16 BEFORE any imports\n",
        "import os\n",
        "os.environ[\"DISABLE_BF16\"] = \"1\"\n",
        "\n",
        "# Check GPU availability and T4 compatibility\n",
        "import torch\n",
        "\n",
        "# Disable TF32 and bf16 at the PyTorch level\n",
        "torch.backends.cuda.matmul.allow_tf32 = False\n",
        "torch.backends.cudnn.allow_tf32 = False\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    compute_capability = torch.cuda.get_device_capability(0)\n",
        "    \n",
        "    print(f\"✅ GPU available: {gpu_name}\")\n",
        "    print(f\"GPU Memory: {gpu_memory:.2f} GB\")\n",
        "    print(f\"Compute Capability: {compute_capability[0]}.{compute_capability[1]}\")\n",
        "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
        "    \n",
        "    # Check if T4 GPU\n",
        "    if \"T4\" in gpu_name:\n",
        "        print(\"\\n⚠️  T4 GPU detected - Important notes:\")\n",
        "        print(\"   - bf16 is NOT supported (will use fp16)\")\n",
        "        print(\"   - 4-bit quantization IS supported\")\n",
        "        print(\"   - Memory: 16GB (suitable for Phi-2 with QLoRA)\")\n",
        "        print(\"\\n✅ T4 optimizations applied (bf16 disabled, fp16 enabled)\")\n",
        "    \n",
        "    # Check compute capability for bitsandbytes\n",
        "    if compute_capability[0] >= 7:\n",
        "        print(\"\\n✅ GPU supports 4-bit quantization (compute capability >= 7.0)\")\n",
        "    else:\n",
        "        print(\"\\n⚠️  GPU may have limited support for 4-bit quantization\")\n",
        "else:\n",
        "    print(\"❌ No GPU detected. Please enable GPU runtime.\")\n",
        "    print(\"Go to: Runtime → Change runtime type → GPU → T4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install all packages in one command to let pip resolve dependencies correctly\n",
        "!pip install -q transformers accelerate datasets bitsandbytes peft trl pyyaml tqdm\n",
        "\n",
        "# Verify installations\n",
        "import torch\n",
        "import transformers\n",
        "import accelerate\n",
        "import bitsandbytes\n",
        "import peft\n",
        "import trl\n",
        "\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"Transformers: {transformers.__version__}\")\n",
        "print(f\"Accelerate: {accelerate.__version__}\")\n",
        "print(f\"Bitsandbytes: {bitsandbytes.__version__}\")\n",
        "print(f\"PEFT: {peft.__version__}\")\n",
        "print(f\"TRL: {trl.__version__}\")\n",
        "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
        "\n",
        "print(\"\\n✅ Dependencies installed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Mount Google Drive (Optional)\n",
        "\n",
        "Mount Google Drive to save checkpoints and model files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Uncomment to mount Google Drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# Set output directory (change to your Drive path if mounted)\n",
        "OUTPUT_DIR = \"./outputs\"\n",
        "# OUTPUT_DIR = \"/content/drive/MyDrive/phi2-grpo-outputs\"  # Uncomment if using Drive\n",
        "\n",
        "print(f\"Output directory: {OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Configuration\n",
        "\n",
        "Set your training hyperparameters here. These are optimized for T4 GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model and Dataset\n",
        "MODEL_NAME = \"microsoft/phi-2\"\n",
        "DATASET_NAME = \"OpenAssistant/oasst1\"\n",
        "LANGUAGE = \"en\"  # Filter for English conversations\n",
        "\n",
        "# Training hyperparameters (optimized for T4 GPU 16GB VRAM - targeting 12-13GB usage)\n",
        "# GRPO generates multiple completions per prompt internally (typically 4-8)\n",
        "BATCH_SIZE = 2  # Increased from 1 to better utilize GPU\n",
        "EVAL_BATCH_SIZE = 8  # Must be divisible by GRPO's default generations\n",
        "GRADIENT_ACCUMULATION_STEPS = 16  # Increased for larger effective batch size (2 * 16 = 32)\n",
        "LEARNING_RATE = 1e-4\n",
        "NUM_EPOCHS = 1  # Keep at 1 for initial training\n",
        "MAX_SEQ_LENGTH = 512  # Increased from 256 for better context\n",
        "MAX_SAMPLES = 5000  # Increased from 1000 for more training data\n",
        "# Note: These settings should use approximately 12-13GB of GPU memory\n",
        "\n",
        "# LoRA parameters (balanced for quality and memory)\n",
        "LORA_R = 16  # Increased from 8 for better model capacity\n",
        "LORA_ALPHA = 32  # Increased from 16\n",
        "LORA_DROPOUT = 0.05\n",
        "\n",
        "# Output\n",
        "OUTPUT_DIR = \"./outputs\"\n",
        "SAVE_STEPS = 500\n",
        "LOGGING_STEPS = 10\n",
        "\n",
        "print(\"✅ Configuration set (optimized for ~12-13GB GPU usage)!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Authenticate with Hugging Face (Optional)\n",
        "\n",
        "If you want to push the model to Hugging Face Hub, authenticate here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "# Uncomment and add your HF token\n",
        "login(token=\"hf_token\")\n",
        "\n",
        "print(\"✅ Hugging Face authentication (if enabled)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Load and Prepare Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load tokenizer\n",
        "print(f\"Loading tokenizer: {MODEL_NAME}\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "print(\"✅ Tokenizer loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "print(f\"Loading dataset: {DATASET_NAME}\")\n",
        "dataset = load_dataset(DATASET_NAME, split=\"train\")\n",
        "\n",
        "if MAX_SAMPLES:\n",
        "    dataset = dataset.select(range(min(MAX_SAMPLES, len(dataset))))\n",
        "    print(f\"Limited to {len(dataset)} samples\")\n",
        "\n",
        "print(f\"✅ Dataset loaded: {len(dataset)} samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract conversation pairs from oasst1 tree structure\n",
        "def extract_conversations(dataset, language=\"en\"):\n",
        "    \"\"\"Extract prompt-completion pairs from oasst1.\"\"\"\n",
        "    conversations = []\n",
        "    \n",
        "    # Filter for language and approved messages\n",
        "    filtered = [\n",
        "        msg for msg in dataset\n",
        "        if msg.get(\"lang\") == language\n",
        "        and not msg.get(\"deleted\", False)\n",
        "        and msg.get(\"review_result\", False)\n",
        "    ]\n",
        "    \n",
        "    message_dict = {msg[\"message_id\"]: msg for msg in filtered}\n",
        "    root_messages = [msg for msg in filtered if msg.get(\"parent_id\") is None]\n",
        "    \n",
        "    def get_thread(message_id, thread=None):\n",
        "        if thread is None:\n",
        "            thread = []\n",
        "        if message_id not in message_dict:\n",
        "            return thread\n",
        "        \n",
        "        msg = message_dict[message_id]\n",
        "        thread.append(msg)\n",
        "        \n",
        "        children = [m for m in filtered if m.get(\"parent_id\") == message_id]\n",
        "        # Handle None values in rank - use 0 if rank is None or missing\n",
        "        children.sort(key=lambda x: (x.get(\"rank\") if x.get(\"rank\") is not None else 0, x.get(\"created_date\", \"\")))\n",
        "        \n",
        "        if children:\n",
        "            return get_thread(children[0][\"message_id\"], thread)\n",
        "        return thread\n",
        "    \n",
        "    for root in root_messages:\n",
        "        thread = get_thread(root[\"message_id\"])\n",
        "        for i in range(len(thread) - 1):\n",
        "            if thread[i][\"role\"] == \"prompter\" and thread[i + 1][\"role\"] == \"assistant\":\n",
        "                conversations.append({\n",
        "                    \"prompt\": thread[i][\"text\"],\n",
        "                    \"completion\": thread[i + 1][\"text\"],\n",
        "                })\n",
        "    \n",
        "    return conversations\n",
        "\n",
        "print(\"Extracting conversation pairs...\")\n",
        "conversations = extract_conversations(dataset, LANGUAGE)\n",
        "print(f\"✅ Extracted {len(conversations)} conversation pairs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Format conversations for training\n",
        "def format_for_training(conversations, tokenizer, max_length=512):\n",
        "    \"\"\"Format conversations for GRPO training.\"\"\"\n",
        "    formatted = []\n",
        "    \n",
        "    for conv in conversations:\n",
        "        prompt = conv[\"prompt\"]\n",
        "        completion = conv[\"completion\"]\n",
        "        \n",
        "        # Format prompt text (what GRPOTrainer expects)\n",
        "        prompt_text = f\"Human: {prompt}\\n\\nAssistant: \"\n",
        "        \n",
        "        # Full text for reference\n",
        "        text = f\"Human: {prompt}\\n\\nAssistant: {completion}\"\n",
        "        \n",
        "        tokenized = tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            max_length=max_length,\n",
        "            padding=False,\n",
        "        )\n",
        "        \n",
        "        prompt_tokenized = tokenizer(\n",
        "            prompt_text,\n",
        "            truncation=True,\n",
        "            max_length=max_length,\n",
        "        )\n",
        "        \n",
        "        # GRPOTrainer requires \"prompt\" field\n",
        "        formatted.append({\n",
        "            \"prompt\": prompt_text,  # Required by GRPOTrainer\n",
        "            \"text\": text,  # Full text for reference\n",
        "            \"input_ids\": tokenized[\"input_ids\"],\n",
        "            \"attention_mask\": tokenized[\"attention_mask\"],\n",
        "            \"prompt_length\": len(prompt_tokenized[\"input_ids\"]),\n",
        "        })\n",
        "    \n",
        "    return formatted\n",
        "\n",
        "print(\"Formatting conversations...\")\n",
        "from datasets import Dataset\n",
        "formatted_data = format_for_training(conversations, tokenizer, MAX_SEQ_LENGTH)\n",
        "train_dataset = Dataset.from_list(formatted_data)\n",
        "\n",
        "# Split into train/validation\n",
        "train_dataset = train_dataset.train_test_split(test_size=0.1, seed=42)\n",
        "print(f\"✅ Training samples: {len(train_dataset['train'])}\")\n",
        "print(f\"✅ Validation samples: {len(train_dataset['test'])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Load Model with 4-bit Quantization and QLoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "import torch\n",
        "\n",
        "# T4 GPU does NOT support bf16 - must use fp16\n",
        "# Setup 4-bit quantization with float16 (NOT bfloat16)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,  # T4 requires float16, NOT bfloat16\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "print(f\"Loading model: {MODEL_NAME} with 4-bit quantization (fp16 for T4 GPU)...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.float16,  # Explicitly set to float16 for T4 GPU\n",
        ")\n",
        "\n",
        "print(\"✅ Base model loaded (using fp16 for T4 compatibility)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare model for k-bit training\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# Setup LoRA\n",
        "lora_config = LoraConfig(\n",
        "    r=LORA_R,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"dense\"],\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# T4 GPU: Ensure ALL parameters are fp16 (not bf16)\n",
        "# Convert any bf16 parameters and buffers to fp16\n",
        "import torch\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    if param.dtype == torch.bfloat16:\n",
        "        param.data = param.data.to(torch.float16)\n",
        "        print(f\"Converted parameter {name} from bf16 to fp16\")\n",
        "\n",
        "for name, buffer in model.named_buffers():\n",
        "    if buffer.dtype == torch.bfloat16:\n",
        "        buffer.data = buffer.data.to(torch.float16)\n",
        "        print(f\"Converted buffer {name} from bf16 to fp16\")\n",
        "\n",
        "# Verify no bf16 tensors remain\n",
        "bf16_params = [name for name, p in model.named_parameters() if p.dtype == torch.bfloat16]\n",
        "bf16_buffers = [name for name, b in model.named_buffers() if b.dtype == torch.bfloat16]\n",
        "\n",
        "if bf16_params or bf16_buffers:\n",
        "    print(f\"WARNING: Found bf16 tensors: {bf16_params + bf16_buffers}\")\n",
        "else:\n",
        "    print(\"✅ All model tensors are fp16 (T4 compatible)\")\n",
        "\n",
        "print(\"✅ Model prepared with QLoRA (fp16 for T4 GPU)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Setup GRPO Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from trl import GRPOTrainer, GRPOConfig\n",
        "\n",
        "# Define reward function for GRPO\n",
        "# This function evaluates the quality of generated completions\n",
        "# For now, using a simple length-based reward - you can replace with a reward model\n",
        "def reward_function(prompts, completions, **kwargs):\n",
        "    \"\"\"\n",
        "    Simple reward function for GRPO training.\n",
        "    Returns rewards for each completion.\n",
        "    You can replace this with a more sophisticated reward model.\n",
        "    \"\"\"\n",
        "    rewards = []\n",
        "    for completion in completions:\n",
        "        # Simple reward: encourage reasonable length (not too short, not too long)\n",
        "        # You can customize this based on your needs\n",
        "        length = len(completion.split())\n",
        "        if length < 5:\n",
        "            reward = -1.0  # Penalize very short responses\n",
        "        elif length > 500:\n",
        "            reward = -0.5  # Slightly penalize very long responses\n",
        "        else:\n",
        "            reward = 1.0  # Reward reasonable length responses\n",
        "        \n",
        "        rewards.append(reward)\n",
        "    \n",
        "    return rewards\n",
        "\n",
        "# Setup GRPO config\n",
        "# Note: GRPOConfig extends TrainingArguments, so it uses the same parameters\n",
        "# IMPORTANT: T4 GPU does NOT support bf16 - must use fp16\n",
        "# IMPORTANT: per_device_eval_batch_size must be divisible by num_generations\n",
        "# IMPORTANT: For T4, we use pure fp16 (not mixed precision) to avoid gradient scaler issues\n",
        "grpo_config = GRPOConfig(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=EVAL_BATCH_SIZE,  # Must be divisible by num_generations\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_steps=100,  # Increased back for better training\n",
        "    logging_steps=LOGGING_STEPS,\n",
        "    save_steps=SAVE_STEPS,\n",
        "    eval_steps=SAVE_STEPS,\n",
        "    eval_strategy=\"steps\",  # Re-enabled evaluation\n",
        "    save_strategy=\"steps\",\n",
        "    save_total_limit=3,  # Save more checkpoints\n",
        "    load_best_model_at_end=True,  # Re-enabled to load best model\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    fp16=False,  # T4 GPU: Disable AMP fp16 (we use pure fp16 instead)\n",
        "    bf16=False,  # T4 GPU: MUST be False (T4 doesn't support bf16)\n",
        "    gradient_checkpointing=True,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    report_to=\"tensorboard\",\n",
        "    remove_unused_columns=False,\n",
        "    use_cpu=False,  # Ensure we're using GPU\n",
        ")\n",
        "\n",
        "# Note: GRPO generation parameters are controlled internally\n",
        "# The trainer will use default generation settings\n",
        "\n",
        "# Initialize GRPO trainer\n",
        "# Note: GRPOTrainer requires reward_funcs parameter\n",
        "# Pass tokenizer via processing_class for proper text generation\n",
        "trainer = GRPOTrainer(\n",
        "    model=model,\n",
        "    processing_class=tokenizer,  # Provide tokenizer for generation\n",
        "    args=grpo_config,\n",
        "    train_dataset=train_dataset[\"train\"],\n",
        "    eval_dataset=train_dataset[\"test\"],  # Re-enabled evaluation dataset\n",
        "    reward_funcs=reward_function,  # Required: reward function(s) for GRPO\n",
        ")\n",
        "\n",
        "# T4 GPU: Verify no gradient scaler (pure fp16 mode, not AMP)\n",
        "print(f\"Trainer using AMP fp16: {trainer.args.fp16}\")\n",
        "print(f\"Trainer using bf16: {trainer.args.bf16}\")\n",
        "if hasattr(trainer, 'scaler') and trainer.scaler is not None:\n",
        "    print(\"⚠️  Gradient scaler detected - this may cause issues\")\n",
        "else:\n",
        "    print(\"✅ No gradient scaler (pure fp16 mode for T4)\")\n",
        "\n",
        "# Final T4 check: Ensure model tensors are still fp16 (not bf16)\n",
        "bf16_found = False\n",
        "for name, param in model.named_parameters():\n",
        "    if param.dtype == torch.bfloat16:\n",
        "        param.data = param.data.to(torch.float16)\n",
        "        bf16_found = True\n",
        "for name, buffer in model.named_buffers():\n",
        "    if buffer.dtype == torch.bfloat16:\n",
        "        buffer.data = buffer.data.to(torch.float16)\n",
        "        bf16_found = True\n",
        "\n",
        "if bf16_found:\n",
        "    print(\"⚠️  Converted remaining bf16 tensors to fp16\")\n",
        "else:\n",
        "    print(\"✅ All tensors confirmed fp16 (T4 ready)\")\n",
        "\n",
        "print(\"\\n✅ GRPO trainer initialized (T4 compatible - pure fp16 mode)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Clear GPU Cache and Prepare for Training\n",
        "\n",
        "Free up GPU memory before training begins."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clear GPU cache before training to free up memory\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "# Check available memory\n",
        "if torch.cuda.is_available():\n",
        "    total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    allocated_memory = torch.cuda.memory_allocated(0) / 1e9\n",
        "    cached_memory = torch.cuda.memory_reserved(0) / 1e9\n",
        "    free_memory = total_memory - allocated_memory\n",
        "    \n",
        "    print(f\"Total GPU Memory: {total_memory:.2f} GB\")\n",
        "    print(f\"Allocated Memory: {allocated_memory:.2f} GB\")\n",
        "    print(f\"Cached Memory: {cached_memory:.2f} GB\")\n",
        "    print(f\"Free Memory: {free_memory:.2f} GB\")\n",
        "    print(\"\\n✅ GPU cache cleared\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Train the Model\n",
        "\n",
        "**Training Configuration (targeting ~12-13GB GPU usage):**\n",
        "- Batch size: 2 (GRPO generates multiple completions per prompt internally)\n",
        "- Max sequence length: 512 tokens\n",
        "- Dataset: 5000 samples (train/eval split)\n",
        "- LoRA rank: 16\n",
        "- Gradient accumulation: 16 steps (effective batch size = 32)\n",
        "- Evaluation: Enabled every 500 steps\n",
        "- Expected GPU usage: 12-13GB out of 16GB available\n",
        "\n",
        "**If you get OOM errors**, reduce in this order:\n",
        "1. `BATCH_SIZE = 1` (most effective)\n",
        "2. `MAX_SEQ_LENGTH = 256` (if still OOM)\n",
        "3. `MAX_SAMPLES = 2000` (if still OOM)\n",
        "\n",
        "This will take several hours. Monitor GPU memory usage in the output above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start training\n",
        "print(\"Starting training...\")\n",
        "\n",
        "train_result = trainer.train()\n",
        "\n",
        "print(\"✅ Training complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save final model\n",
        "print(\"Saving model...\")\n",
        "trainer.save_model()\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "\n",
        "# Save training metrics\n",
        "metrics = train_result.metrics\n",
        "trainer.log_metrics(\"train\", metrics)\n",
        "trainer.save_metrics(\"train\", metrics)\n",
        "\n",
        "print(f\"✅ Model saved to {OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Test the Model (Inference)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the model\n",
        "model.eval()\n",
        "\n",
        "test_prompt = \"What is machine learning?\"\n",
        "formatted_prompt = f\"Human: {test_prompt}\\n\\nAssistant: \"\n",
        "\n",
        "inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=256,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(f\"Prompt: {test_prompt}\")\n",
        "print(f\"Response: {response.split('Assistant:')[-1].strip()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Push to Hugging Face Hub (Optional)\n",
        "\n",
        "Uncomment and set your model ID to push the model to Hugging Face Hub."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment to push to Hub\n",
        "# HUB_MODEL_ID = \"your-username/phi2-grpo-finetuned\"\n",
        "# trainer.push_to_hub(hub_model_id=HUB_MODEL_ID)\n",
        "# print(f\"✅ Model pushed to https://huggingface.co/{HUB_MODEL_ID}\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
